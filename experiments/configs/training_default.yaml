# Default training configuration for quadcopter controller
# This configuration provides sensible defaults for getting started with training.

# Controller selection
# Options: deep, lqr, pid
# - deep: Neural network controller (requires training)
# - lqr: Linear Quadratic Regulator (no training, evaluation only)
# - pid: PID controller (no training, evaluation only)
controller: deep

# Training parameters
epochs: 100
episodes_per_epoch: 10
max_steps_per_episode: 3000
batch_size: 32

# Optimizer parameters
learning_rate: 0.001
optimizer: adam  # Options: adam, sgd, adamw
weight_decay: 0.0
grad_clip: 1.0

# Network architecture
hidden_sizes: [64, 64]
activation: relu  # Options: relu, tanh, elu, leaky_relu

# Loss function weights
position_weight: 1.0
velocity_weight: 0.1
control_weight: 0.01
error_type: l2  # Options: l2, l1, huber
tracking_weight: 1.0
reward_weight: 0.0  # Set > 0 to include reward shaping

# =============================================================================
# Training Mode Configuration
# =============================================================================
# training_mode: Determines the learning signal used for training
# Options:
#   - tracking: Pure tracking loss (position/velocity error) - default
#   - imitation: Learn from PID/LQR supervisor actions
#   - reward_weighted: Combine tracking loss with supervisor hints
training_mode: tracking

# supervisor_controller: Classical controller to generate target actions
# Options: pid, lqr
# Used when training_mode is 'imitation' or 'reward_weighted'
supervisor_controller: pid

# imitation_weight: Weight for imitation loss when training_mode='imitation'
# Higher values make the policy follow supervisor more closely
imitation_weight: 1.0

# supervisor_blend_ratio: How much to blend supervisor actions (future use)
# 0.0 = use policy actions only, 1.0 = use supervisor actions only
supervisor_blend_ratio: 0.0

# Environment parameters
env_seed: 42
target_motion_type: circular  # Options: linear, circular, sinusoidal, figure8, stationary
episode_length: 30.0  # seconds
target_radius: 0.5  # meters

# Checkpointing
checkpoint_dir: checkpoints
checkpoint_interval: 10
save_best: true

# Logging
log_dir: experiments/logs
log_interval: 1

# Curriculum learning (optional)
use_curriculum: false
curriculum_start_difficulty: 0.5
curriculum_end_difficulty: 1.0

# Recovery settings for training stability
nan_recovery_attempts: 3
lr_reduction_factor: 0.5

# Device (auto-detected if not specified)
# device: cpu

# =============================================================================
# Classical Controller Configuration (PID and LQR)
# =============================================================================
# These settings are used when evaluating PID or LQR controllers.
# Validated baseline gains with low XY proportional gains to prevent actuator
# saturation (meter→rad/s mapping). Integral term disabled by default.

# PID Controller Gains
# Tuned for stable tracking across stationary, linear, and circular scenarios.
# XY gains are intentionally small because position errors (in meters) are
# mapped to angular rates (in rad/s). A 1-meter XY error with kp=0.01 produces
# only 0.01 rad/s of pitch/roll rate, preventing saturation while converging.
pid:
  # Position proportional gains [x, y, z] - low XY for meter→rad/s mapping
  kp_pos: [0.01, 0.01, 4.0]
  # Position integral gains [x, y, z] - zero to avoid wind-up
  ki_pos: [0.0, 0.0, 0.0]
  # Position derivative gains [x, y, z] - low XY damping
  kd_pos: [0.06, 0.06, 2.0]
  # Integral windup limit - disabled by default
  integral_limit: 0.0
  # Feedforward configuration (optional, disabled by default)
  # Enable for improved tracking of moving targets
  feedforward_enabled: false
  # Feedforward gains for target velocity [x, y, z] (m/s -> control units)
  ff_velocity_gain: [0.0, 0.0, 0.0]
  # Feedforward gains for target acceleration [x, y, z] (m/s² -> control units)
  ff_acceleration_gain: [0.0, 0.0, 0.0]
  # Max velocity/acceleration for clamping (prevents oscillation from noise)
  ff_max_velocity: 10.0
  ff_max_acceleration: 5.0

# LQR Controller Gains
# Cost weights scaled to produce feedback gains consistent with PID baseline.
# Low XY position costs prevent saturation due to meter→rad/s mapping.
lqr:
  # Position state cost weights [x, y, z] - low XY costs
  q_pos: [0.0001, 0.0001, 16.0]
  # Velocity state cost weights [vx, vy, vz] - provides damping
  q_vel: [0.0036, 0.0036, 4.0]
  # Thrust control cost
  r_thrust: 1.0
  # Angular rate control cost
  r_rate: 1.0
  # Feedforward configuration (optional, disabled by default)
  # Enable for improved tracking of moving targets
  feedforward_enabled: false
  # Feedforward gains for target velocity [x, y, z]
  ff_velocity_gain: [0.0, 0.0, 0.0]
  # Feedforward gains for target acceleration [x, y, z]
  ff_acceleration_gain: [0.0, 0.0, 0.0]
  # Max velocity/acceleration for clamping
  ff_max_velocity: 10.0
  ff_max_acceleration: 5.0
